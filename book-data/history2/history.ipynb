{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "188c9aea-7441-438b-930b-493af291704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据中...\n",
      "分析数据中...\n",
      "总评论数: 2066193\n",
      "总用户数: 238450\n",
      "总书籍数: 302346\n",
      "\n",
      "用户评论数分析:\n",
      "平均每个用户的评论数: 8.67\n",
      "最少评论数: 1, 最多评论数: 3368\n",
      "\n",
      "书籍评论数分析:\n",
      "平均每本书的评论数: 6.83\n",
      "最少评论数: 1, 最多评论数: 11300\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取 Goodreads 子类数据\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# 统计基本信息\n",
    "def analyze_reviews(reviews):\n",
    "    user_count = defaultdict(int)\n",
    "    book_count = defaultdict(int)\n",
    "    \n",
    "    # 统计每个用户的评论次数和每本书的评论次数\n",
    "    for review in reviews:\n",
    "        user_id = review['user_id']\n",
    "        book_id = review['book_id']\n",
    "        user_count[user_id] += 1\n",
    "        book_count[book_id] += 1\n",
    "    \n",
    "    total_reviews = len(reviews)\n",
    "    total_users = len(user_count)\n",
    "    total_books = len(book_count)\n",
    "    \n",
    "    print(f\"总评论数: {total_reviews}\")\n",
    "    print(f\"总用户数: {total_users}\")\n",
    "    print(f\"总书籍数: {total_books}\")\n",
    "    \n",
    "    # 分析评论次数分布\n",
    "    user_review_counts = list(user_count.values())\n",
    "    book_review_counts = list(book_count.values())\n",
    "    \n",
    "    print(f\"\\n用户评论数分析:\")\n",
    "    print(f\"平均每个用户的评论数: {sum(user_review_counts) / len(user_review_counts):.2f}\")\n",
    "    print(f\"最少评论数: {min(user_review_counts)}, 最多评论数: {max(user_review_counts)}\")\n",
    "    \n",
    "    print(f\"\\n书籍评论数分析:\")\n",
    "    print(f\"平均每本书的评论数: {sum(book_review_counts) / len(book_review_counts):.2f}\")\n",
    "    print(f\"最少评论数: {min(book_review_counts)}, 最多评论数: {max(book_review_counts)}\")\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_history_biography.json.gz'\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "reviews = load_reviews(file_path)\n",
    "\n",
    "print(\"分析数据中...\")\n",
    "analyze_reviews(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcf74b9-8d38-4c32-beff-19c9b57e0b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印第一个 JSON 对象中的特征:\n",
      "{\n",
      "  \"user_id\": \"8842281e1d1347389f2ab93d60773d4d\",\n",
      "  \"book_id\": \"29893493\",\n",
      "  \"review_id\": \"c23406fb584d6304d1dd4c75ce26ea3a\",\n",
      "  \"rating\": 5,\n",
      "  \"review_text\": \"I haven't read a non-fiction book this engaging in some time. This was an amazingly well written autobiography. It read like a fast paced novel through much of it. Many autobiographies are too long - this one if anything is too short! Like many startup companies, the story of Nike (or Blue Ribbon Shoes as it was initially called) is one of trials, tribulations, and lots of passion and grit. I ate it up, and highly recommend it. \\n The most defining moment of the story is Knights ballsy move while backpacking through Japan at the age of 24, to walk into a Japanese shoe manufacturer and say he has a shoe distribution company, and get a exclusive deal for the western US. Gutsy. He \\\"just did it\\\" (sorry, but apt). I love learning examples of this kind of \\\"Do things that don't scale\\\" start to successful companies (eg Zappos, Amazon, many more). From there the story is one of doubling sales each year, and never quite having enough money on the balance sheet to make it anything but very risky. It was interesting - no fascinating - that Phil had an accounting background, and was well versed that the reason most startups fail is a lack of cash reserves, and yet he had so much faith in his growth and sales that he kept plowing all cash into growth. I loved reading the stories of how they barely made it from one order to the next, how twice they had to switch banks after being cut off. \\n One of the key strategies of Nike's success today is athlete endorsement, and it was interesting to see how that strategy was formed in the early years. How they would offer large sums of money to athletes to wear their shoes - often to athletes already wearing their shoes - double down on people who already like your product. It was doubly interesting that Phil didn't initially believe in the power of advertising - I'd be very curious to hear how his opinion on that changed over time. \\n The story of Prefontaine was a touching one, and one that stayed with me. I wasn't as familiar of the story of Pre, but had heard of him. But I didn't know the role that Nike sponsored him, and even employed him. Reading Phil talk about him, you got a sense of the passion his has for the sport of running. You got a sense that the story of Pre - his passion and drive - is a metaphor for how Nike was built. This quote says it better: \\n \\\"For eleven laps they ran a half stride apart. With the crowd now roaring, frothing, shrieking, the two men entered the final lap. It felt like a boxing match. It felt like a joust. It felt like a bullfight, and we were down to that moment of truth--death hanging in the air. Pre reached down, found another level--we saw him do it. He opened up a yard lead, then two, then five. We saw Young grimacing and we knew that he could not, would not, catch Pre. I told myself, Don't forget this. Do not forget. I told myself there was much to be learned from such a display of passion, whether you were running a mile or a company.\\\" \\n The book is a fascinating telling of the founding and early history of Nike. But it stops at the IPO, and then gives a chapter postlude. This is my only complaint. It was so well written - keep going! Tell us the story of Just Do It, Air Jordans, and so many more. I could read 4 more volumes of this, as it feels like there is so much more to they story of Nike to learn.\",\n",
      "  \"date_added\": \"Thu Dec 15 10:51:26 -0800 2016\",\n",
      "  \"date_updated\": \"Sun Mar 12 23:33:51 -0700 2017\",\n",
      "  \"read_at\": \"Thu Mar 09 15:34:06 -0800 2017\",\n",
      "  \"started_at\": \"Tue Feb 28 17:55:35 -0800 2017\",\n",
      "  \"n_votes\": 29,\n",
      "  \"n_comments\": 8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "# 读取 Goodreads 子类数据的一个 JSON 对象\n",
    "def print_first_review(file_path):\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        first_line = f.readline()  # 读取第一行数据\n",
    "        first_review = json.loads(first_line)  # 将其转为 JSON 格式\n",
    "        print(json.dumps(first_review, indent=2))  # 格式化输出 JSON 对象\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_history_biography.json.gz'\n",
    "\n",
    "# 打印第一个 JSON 对象\n",
    "print(\"打印第一个 JSON 对象中的特征:\")\n",
    "print_first_review(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d88a964b-ea42-4288-a48a-bc5b20a92d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据中...\n",
      "最受欢迎的 1608 本书的评论次数分布:\n",
      "书籍ID: 19063, 评论次数: 11300\n",
      "书籍ID: 4667024, 评论次数: 8239\n",
      "书籍ID: 18143977, 评论次数: 7419\n",
      "书籍ID: 2657, 评论次数: 7342\n",
      "书籍ID: 43641, 评论次数: 6091\n",
      "书籍ID: 10964, 评论次数: 5775\n",
      "书籍ID: 21853621, 评论次数: 4564\n",
      "书籍ID: 10644930, 评论次数: 4267\n",
      "书籍ID: 2728527, 评论次数: 4257\n",
      "书籍ID: 7445, 评论次数: 3937\n",
      "\n",
      "用户在前1608本书中的阅读情况:\n",
      "用户ID: 8842281e1d1347389f2ab93d60773d4d, 阅读涉及的4000本书中的数量: 23\n",
      "用户ID: 72fb0d0087d28c832f15776b0d936598, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: d986f354a045ffb91234e4af4d1b12fd, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 704eb93a316aff687a93d5215882eb21, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: f4d16ea4ac59af59d257631398af39f4, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 4b3636a043e5c99fa27ac897ccfa1151, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 903d4b859e86a1dd6d7640849cc7067c, 阅读涉及的4000本书中的数量: 2\n",
      "用户ID: afc070543f19028dc7e7f084a0079f72, 阅读涉及的4000本书中的数量: 6\n",
      "用户ID: d92c94bda1ca3ec254f3aa95757c7831, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 7b2e5fe9fd353fecf3eeebb4850b88d3, 阅读涉及的4000本书中的数量: 3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取 Goodreads 子类数据\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# 选出最受欢迎的前4000本书\n",
    "def select_top_books(reviews, top_n=2608):\n",
    "    book_count = defaultdict(int)\n",
    "    \n",
    "    # 统计每本书的评论次数\n",
    "    for review in reviews:\n",
    "        book_id = review['book_id']\n",
    "        book_count[book_id] += 1\n",
    "    \n",
    "    # 按评论次数排序，选出前 top_n 本书\n",
    "    top_books = sorted(book_count.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_book_ids = set([book_id for book_id, count in top_books])\n",
    "    \n",
    "    return top_books, top_book_ids\n",
    "\n",
    "# 统计每个用户在前4000本书中的阅读情况\n",
    "def analyze_user_reading(reviews, top_book_ids):\n",
    "    user_top_book_count = defaultdict(int)\n",
    "    \n",
    "    # 按用户统计其阅读的前4000本书的数量\n",
    "    for review in reviews:\n",
    "        user_id = review['user_id']\n",
    "        book_id = review['book_id']\n",
    "        if book_id in top_book_ids:\n",
    "            user_top_book_count[user_id] += 1\n",
    "    \n",
    "    return user_top_book_count\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_history_biography.json.gz'\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "reviews = load_reviews(file_path)\n",
    "\n",
    "# 选出最受欢迎的前4000本书\n",
    "top_books, top_book_ids = select_top_books(reviews, top_n=1608)\n",
    "\n",
    "# 输出最受欢迎的书籍及其评论次数\n",
    "print(f\"最受欢迎的 1608 本书的评论次数分布:\")\n",
    "for book_id, count in top_books[:10]:  # 仅输出前10本书的评论数\n",
    "    print(f\"书籍ID: {book_id}, 评论次数: {count}\")\n",
    "\n",
    "# 统计每个用户在这 4000 本书中阅读的数量\n",
    "user_top_book_count = analyze_user_reading(reviews, top_book_ids)\n",
    "\n",
    "# 输出部分用户阅读情况\n",
    "print(f\"\\n用户在前1608本书中的阅读情况:\")\n",
    "for user_id, count in list(user_top_book_count.items())[:10]:  # 仅输出前10个用户\n",
    "    print(f\"用户ID: {user_id}, 阅读涉及的4000本书中的数量: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3a9fa99-fe1f-47f2-ab84-e1101c16046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读最多书的用户ID: 7b82d02a42678fbdaaee5e119981bdb8, 阅读的4000本热门书中的数量: 254\n",
      "读最少书的用户ID: c0bf4b6a130946fe3f1a8e45ec372816, 阅读的4000本热门书中的数量: 46\n",
      "\n",
      "前 1120 用户在4000本热门书中的阅读情况:\n",
      "用户ID: cf0bacb5c3718016ce15c43943401b20, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 6792b018843768c7e72a7975127289b2, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 4add8c6a4c47e3eeebf93bfba72848b3, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 098f71df8945d3b3284dc2d0d0505724, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 00cdd405cd06c9d64ac273624252f00b, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 3057b4ba46a03796e64853a8682efbdb, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 559e950b58080e646b733dd2d305a0e2, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 735ad9df127bb15ba1c59fd2f5c00480, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: 31b825f2b542020399e2e9706779cbe8, 阅读涉及的4000本书中的数量: 46\n",
      "用户ID: c0bf4b6a130946fe3f1a8e45ec372816, 阅读涉及的4000本书中的数量: 46\n"
     ]
    }
   ],
   "source": [
    "# 按照用户阅读的热门书籍数量排序，并选出前 1000 个用户\n",
    "def get_top_n_users(user_top_book_count, top_n=1120):\n",
    "    sorted_users = sorted(user_top_book_count.items(), key=lambda x: x[1], reverse=True)  # 按照阅读量从大到小排序\n",
    "    top_users = sorted_users[:top_n]  # 选出前 top_n 个用户\n",
    "    return top_users\n",
    "\n",
    "# 显示读最多和读最少的用户的阅读量\n",
    "def display_min_max_user_reading(top_users):\n",
    "    max_read_user = top_users[0]\n",
    "    min_read_user = top_users[-1]\n",
    "    \n",
    "    print(f\"读最多书的用户ID: {max_read_user[0]}, 阅读的4000本热门书中的数量: {max_read_user[1]}\")\n",
    "    print(f\"读最少书的用户ID: {min_read_user[0]}, 阅读的4000本热门书中的数量: {min_read_user[1]}\")\n",
    "\n",
    "# 筛选出前 1000 个用户\n",
    "top_n = 1120\n",
    "top_users = get_top_n_users(user_top_book_count, top_n=top_n)\n",
    "\n",
    "# 显示读最多书和读最少书的用户的阅读量\n",
    "display_min_max_user_reading(top_users)\n",
    "\n",
    "# 输出前1000个用户的阅读情况\n",
    "print(f\"\\n前 {top_n} 用户在4000本热门书中的阅读情况:\")\n",
    "for user_id, count in top_users[1110:1120]:  # 仅显示前10个用户的阅读情况\n",
    "    print(f\"用户ID: {user_id}, 阅读涉及的4000本书中的数量: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e6a9537-6320-4adf-8d8d-872abe6dcae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying top 4000 books...\n",
      "Analyzing user reading patterns...\n",
      "Selecting top 1000 users...\n",
      "Processing and saving data...\n",
      "\n",
      "Top 4000 books:\n",
      "Most reviewed book: 11300 reviews\n",
      "Least reviewed book among top 4000: 92 reviews\n",
      "\n",
      "Top 1000 users:\n",
      "User with most books read: 320 books\n",
      "User with least books read among top 1000: 55 books\n",
      "\n",
      "Processed data saved to /workspace/history/processed_ba_user_sessions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import heapq\n",
    "\n",
    "def parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_string, \"%a %b %d %H:%M:%S %z %Y\").timestamp()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def get_top_books(file_path, top_n=2608):\n",
    "    book_counts = Counter()\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            book_counts[review['book_id']] += 1\n",
    "    return dict(book_counts.most_common(top_n))\n",
    "\n",
    "def get_user_reading_counts(file_path, top_books):\n",
    "    user_reading_counts = defaultdict(int)\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            if review['book_id'] in top_books:\n",
    "                user_reading_counts[review['user_id']] += 1\n",
    "    return user_reading_counts\n",
    "\n",
    "def get_top_users(user_reading_counts, top_n=1120):\n",
    "    return dict(sorted(user_reading_counts.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "def load_book_titles(file_path):\n",
    "    book_titles = {}\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            book = json.loads(line)\n",
    "            book_titles[book['book_id']] = book['title']\n",
    "    return book_titles\n",
    "\n",
    "def process_and_save_data(reviews_file, books_file, output_file, top_books, top_users):\n",
    "    book_titles = load_book_titles(books_file)\n",
    "    user_data = defaultdict(list)\n",
    "    \n",
    "    with gzip.open(reviews_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            user_id = review['user_id']\n",
    "            book_id = review['book_id']\n",
    "            \n",
    "            if user_id in top_users and book_id in top_books:\n",
    "                read_at = parse_date(review.get('read_at')) or parse_date(review.get('date_added'))\n",
    "                title = book_titles.get(book_id, \"Unknown Title\")\n",
    "                user_data[user_id].append({\n",
    "                    'book_id': book_id,\n",
    "                    'title': title,\n",
    "                    'read_at': read_at\n",
    "                })\n",
    "    \n",
    "    # Sort each user's reading list by date\n",
    "    for user_id, books in user_data.items():\n",
    "        user_data[user_id] = sorted(books, key=lambda x: x['read_at'] or 0)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(user_data, f)\n",
    "\n",
    "# Main processing\n",
    "reviews_file = '/workspace/goodreads/goodreads_reviews_history_biography.json.gz'\n",
    "books_file = '/workspace/goodreads/goodreads_books.json.gz'\n",
    "output_file = '/workspace/history/processed_ba_user_sessions.json'\n",
    "\n",
    "print(\"Identifying top 4000 books...\")\n",
    "top_books = get_top_books(reviews_file)\n",
    "\n",
    "print(\"Analyzing user reading patterns...\")\n",
    "user_reading_counts = get_user_reading_counts(reviews_file, top_books)\n",
    "\n",
    "print(\"Selecting top 1000 users...\")\n",
    "top_users = get_top_users(user_reading_counts)\n",
    "\n",
    "print(\"Processing and saving data...\")\n",
    "process_and_save_data(reviews_file, books_file, output_file, top_books, top_users)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTop 4000 books:\")\n",
    "print(f\"Most reviewed book: {max(top_books.values())} reviews\")\n",
    "print(f\"Least reviewed book among top 4000: {min(top_books.values())} reviews\")\n",
    "\n",
    "print(f\"\\nTop 1000 users:\")\n",
    "print(f\"User with most books read: {max(top_users.values())} books\")\n",
    "print(f\"User with least books read among top 1000: {min(top_users.values())} books\")\n",
    "\n",
    "print(f\"\\nProcessed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0f43f4e-0b3f-44c7-b97e-8e1f6bae8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数: 1120\n",
      "平均阅读量: 82.03\n",
      "中位数阅读量: 72\n",
      "最小阅读量: 55\n",
      "最大阅读量: 320\n",
      "\n",
      "平均阅读时间跨度: 4488.00 天\n",
      "\n",
      "最受欢迎的10本书:\n",
      "The Help: 557 次阅读\n",
      "All the Light We Cannot See: 541 次阅读\n",
      "The Book Thief: 537 次阅读\n",
      "The Guernsey Literary and Potato Peel Pie Society: 462 次阅读\n",
      "The Immortal Life of Henrietta Lacks: 421 次阅读\n",
      "The Nightingale: 405 次阅读\n",
      "Unbroken: A World War II Story of Survival, Resilience, and Redemption: 395 次阅读\n",
      "The Light Between Oceans: 393 次阅读\n",
      "Bossypants: 366 次阅读\n",
      "Water for Elephants: 363 次阅读\n",
      "\n",
      "随机用户 df2578d3aee06a554fd389c55dcf435a 的前5条阅读记录:\n",
      "书名: Katherine, 阅读时间: 2011-03-18 07:00:00\n",
      "书名: The Bonesetter's Daughter, 阅读时间: 2011-04-06 03:29:39\n",
      "书名: The Pillars of the Earth (Kingsbridge, #1), 阅读时间: 2011-04-06 03:32:22\n",
      "书名: The 19th Wife, 阅读时间: 2011-04-06 03:32:54\n",
      "书名: The Russian Concubine (The Russian Concubine, #1), 阅读时间: 2011-04-07 00:46:26\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_data(data):\n",
    "    user_book_counts = {user: len(books) for user, books in data.items()}\n",
    "    reading_spans = {}\n",
    "    books_counter = Counter()\n",
    "\n",
    "    for user, books in data.items():\n",
    "        if books:\n",
    "            dates = [book['read_at'] for book in books if book['read_at']]\n",
    "            if dates:\n",
    "                reading_spans[user] = max(dates) - min(dates)\n",
    "        books_counter.update([book['title'] for book in books])\n",
    "\n",
    "    return user_book_counts, reading_spans, books_counter\n",
    "\n",
    "def print_statistics(user_book_counts, reading_spans, books_counter):\n",
    "    print(f\"用户数: {len(user_book_counts)}\")\n",
    "    print(f\"平均阅读量: {sum(user_book_counts.values()) / len(user_book_counts):.2f}\")\n",
    "    print(f\"中位数阅读量: {sorted(user_book_counts.values())[len(user_book_counts)//2]}\")\n",
    "    print(f\"最小阅读量: {min(user_book_counts.values())}\")\n",
    "    print(f\"最大阅读量: {max(user_book_counts.values())}\")\n",
    "\n",
    "    avg_span = sum(reading_spans.values()) / len(reading_spans) / (24 * 3600)  # 转换为天\n",
    "    print(f\"\\n平均阅读时间跨度: {avg_span:.2f} 天\")\n",
    "\n",
    "    print(\"\\n最受欢迎的10本书:\")\n",
    "    for book, count in books_counter.most_common(10):\n",
    "        print(f\"{book}: {count} 次阅读\")\n",
    "\n",
    "    # 随机选择一个用户进行样本检查\n",
    "    sample_user = random.choice(list(data.keys()))\n",
    "    print(f\"\\n随机用户 {sample_user} 的前5条阅读记录:\")\n",
    "    for book in data[sample_user][:5]:\n",
    "        print(f\"书名: {book['title']}, 阅读时间: {datetime.fromtimestamp(book['read_at'])}\")\n",
    "\n",
    "# 主处理流程\n",
    "file_path = '/workspace/history/processed_ba_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "user_book_counts, reading_spans, books_counter = analyze_data(data)\n",
    "print_statistics(user_book_counts, reading_spans, books_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b0d195f-6c4a-4ba9-a3fc-c2699de588aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "调试信息：前5个用户和其书籍映射结果\n",
      "新用户ID: 0, 阅读记录数: 90\n",
      "  新书籍ID: 0, 书名: Affinity, 阅读时间: 1341371308.0\n",
      "  新书籍ID: 1, 书名: Elizabeth Street, 阅读时间: 1342162800.0\n",
      "  新书籍ID: 2, 书名: A Spy in the House (The Agency, #1), 阅读时间: 1351580400.0\n",
      "  新书籍ID: 3, 书名: The Body at the Tower (The Agency, #2), 阅读时间: 1351839600.0\n",
      "  新书籍ID: 4, 书名: The Traitor in the Tunnel (The Agency, #3), 阅读时间: 1352012400.0\n",
      "新用户ID: 1, 阅读记录数: 55\n",
      "  新书籍ID: 90, 书名: Atonement, 阅读时间: 1248850800.0\n",
      "  新书籍ID: 91, 书名: Roll of Thunder, Hear My Cry (Logans, #4), 阅读时间: 1251788400.0\n",
      "  新书籍ID: 92, 书名: Out of the Dust, 阅读时间: 1259740800.0\n",
      "  新书籍ID: 93, 书名: The Guernsey Literary and Potato Peel Pie Society, 阅读时间: 1273042800.0\n",
      "  新书籍ID: 94, 书名: Prayers for Sale, 阅读时间: 1273906800.0\n",
      "新用户ID: 2, 阅读记录数: 60\n",
      "  新书籍ID: 141, 书名: The Wedding (Lairds' Fiancées, #2), 阅读时间: 874306800.0\n",
      "  新书籍ID: 142, 书名: sTORI Telling, 阅读时间: 1239260400.0\n",
      "  新书籍ID: 143, 书名: A Reliable Wife, 阅读时间: 1246690800.0\n",
      "  新书籍ID: 144, 书名: The Seance, 阅读时间: 1247554800.0\n",
      "  新书籍ID: 145, 书名: To Tame a Highland Warrior (Highlander, #2), 阅读时间: 1256713200.0\n",
      "新用户ID: 3, 阅读记录数: 60\n",
      "  新书籍ID: 120, 书名: The Secret Keeper, 阅读时间: 1355040000.0\n",
      "  新书籍ID: 194, 书名: Let's Pretend This Never Happened: A Mostly True Memoir, 阅读时间: 1356508800.0\n",
      "  新书籍ID: 158, 书名: The Winter Sea (Slains, #1), 阅读时间: 1369033200.0\n",
      "  新书籍ID: 98, 书名: To Kill a Mockingbird, 阅读时间: 1376228266.0\n",
      "  新书籍ID: 195, 书名: Bonhoeffer: Pastor, Martyr, Prophet, Spy, 阅读时间: 1377500400.0\n",
      "新用户ID: 4, 阅读记录数: 71\n",
      "  新书籍ID: 235, 书名: The Thorn Birds, 阅读时间: 662716800.0\n",
      "  新书籍ID: 236, 书名: The Princes of Ireland (The Dublin Saga, #1), 阅读时间: 1183273200.0\n",
      "  新书籍ID: 237, 书名: Zorro, 阅读时间: 1193900400.0\n",
      "  新书籍ID: 90, 书名: Atonement, 阅读时间: 1199174400.0\n",
      "  新书籍ID: 39, 书名: The Secret History of the Pink Carnation (Pink Carnation, #1), 阅读时间: 1201852800.0\n",
      "\n",
      "调试信息：前5个用户ID映射\n",
      "旧用户ID: 792500e85277fa7ada535de23e7eb4c3, 新用户ID: 0\n",
      "旧用户ID: fc0a0792fd1c30427acdbfecbf5b0a20, 新用户ID: 1\n",
      "旧用户ID: ab2fadb5c7bbe55c80406d2b3692e969, 新用户ID: 2\n",
      "旧用户ID: 3b3f26019b3a5dbecb49c5faf1abce4c, 新用户ID: 3\n",
      "旧用户ID: d6804aa6e3a96b8e1104b8b9ac3fe882, 新用户ID: 4\n",
      "\n",
      "调试信息：前5个书籍标题映射\n",
      "书名: Affinity, 新书籍ID: 0\n",
      "书名: Elizabeth Street, 新书籍ID: 1\n",
      "书名: A Spy in the House (The Agency, #1), 新书籍ID: 2\n",
      "书名: The Body at the Tower (The Agency, #2), 新书籍ID: 3\n",
      "书名: The Traitor in the Tunnel (The Agency, #3), 新书籍ID: 4\n",
      "处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def remap_ids(data):\n",
    "    user_id_map = {}\n",
    "    book_title_map = {}  # 使用书名作为键\n",
    "    new_user_id = 0\n",
    "    new_book_id = 0\n",
    "    \n",
    "    remapped_data = {}\n",
    "    for user, books in data.items():\n",
    "        if user not in user_id_map:\n",
    "            user_id_map[user] = new_user_id\n",
    "            new_user_id += 1\n",
    "        \n",
    "        new_books = []\n",
    "        for book in books:\n",
    "            title = book['title']\n",
    "            # 如果书名没有映射过，分配新的ID\n",
    "            if title not in book_title_map:\n",
    "                book_title_map[title] = new_book_id\n",
    "                new_book_id += 1\n",
    "            \n",
    "            # 替换书籍ID\n",
    "            new_books.append({\n",
    "                'book_id': book_title_map[title],\n",
    "                'title': title,\n",
    "                'read_at': book['read_at']\n",
    "            })\n",
    "        \n",
    "        remapped_data[user_id_map[user]] = new_books\n",
    "\n",
    "    # 调试信息\n",
    "    print(\"\\n调试信息：前5个用户和其书籍映射结果\")\n",
    "    for i, (user, books) in enumerate(remapped_data.items()):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"新用户ID: {user}, 阅读记录数: {len(books)}\")\n",
    "        for book in books[:5]:\n",
    "            print(f\"  新书籍ID: {book['book_id']}, 书名: {book['title']}, 阅读时间: {book['read_at']}\")\n",
    "    \n",
    "    return remapped_data, user_id_map, book_title_map\n",
    "\n",
    "def generate_id2name(book_title_map, output_file):\n",
    "    sorted_books = sorted(book_title_map.items(), key=lambda x: x[1])\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for title, new_id in sorted_books:\n",
    "            f.write(f\"{new_id}:: {title}\\n\")\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/history/processed_ba_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "\n",
    "remapped_data, user_id_map, book_title_map = remap_ids(data)\n",
    "\n",
    "with open('/workspace/history/remapped_ya_user_sessions.json', 'w') as f:\n",
    "    json.dump(remapped_data, f, indent=2)\n",
    "\n",
    "generate_id2name(book_title_map, '/workspace/history/id2name.txt')\n",
    "\n",
    "with open('/workspace/history/user_id_map.json', 'w') as f:\n",
    "    json.dump(user_id_map, f, indent=2)\n",
    "\n",
    "# 调试信息\n",
    "print(\"\\n调试信息：前5个用户ID映射\")\n",
    "for i, (old_id, new_id) in enumerate(user_id_map.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"旧用户ID: {old_id}, 新用户ID: {new_id}\")\n",
    "\n",
    "print(\"\\n调试信息：前5个书籍标题映射\")\n",
    "for i, (title, new_id) in enumerate(book_title_map.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"书名: {title}, 新书籍ID: {new_id}\")\n",
    "\n",
    "print(\"处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc4d37e4-f364-488f-9a25-6f77e332ff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "调试信息：前5个用户和其书籍映射结果\n",
      "新用户ID: 0, 阅读记录数: 69\n",
      "  新书籍ID: 0, 书名: Affinity, 阅读时间: 1341371308.0\n",
      "  新书籍ID: 1, 书名: A Spy in the House (The Agency, #1), 阅读时间: 1351580400.0\n",
      "  新书籍ID: 2, 书名: The Body at the Tower (The Agency, #2), 阅读时间: 1351839600.0\n",
      "  新书籍ID: 3, 书名: The Cater Street Hangman (Charlotte & Thomas Pitt, #1), 阅读时间: 1360170255.0\n",
      "  新书籍ID: 4, 书名: The Apothecary's Daughter, 阅读时间: 1360742400.0\n",
      "新用户ID: 1, 阅读记录数: 50\n",
      "  新书籍ID: 69, 书名: Atonement, 阅读时间: 1248850800.0\n",
      "  新书籍ID: 70, 书名: Roll of Thunder, Hear My Cry (Logans, #4), 阅读时间: 1251788400.0\n",
      "  新书籍ID: 71, 书名: Out of the Dust, 阅读时间: 1259740800.0\n",
      "  新书籍ID: 72, 书名: The Guernsey Literary and Potato Peel Pie Society, 阅读时间: 1273042800.0\n",
      "  新书籍ID: 73, 书名: Prayers for Sale, 阅读时间: 1273906800.0\n",
      "新用户ID: 2, 阅读记录数: 49\n",
      "  新书籍ID: 116, 书名: The Wedding (Lairds' Fiancées, #2), 阅读时间: 874306800.0\n",
      "  新书籍ID: 117, 书名: sTORI Telling, 阅读时间: 1239260400.0\n",
      "  新书籍ID: 118, 书名: A Reliable Wife, 阅读时间: 1246690800.0\n",
      "  新书籍ID: 119, 书名: To Tame a Highland Warrior (Highlander, #2), 阅读时间: 1256713200.0\n",
      "  新书籍ID: 72, 书名: The Guernsey Literary and Potato Peel Pie Society, 阅读时间: 1261641600.0\n",
      "新用户ID: 3, 阅读记录数: 55\n",
      "  新书籍ID: 95, 书名: The Secret Keeper, 阅读时间: 1355040000.0\n",
      "  新书籍ID: 160, 书名: Let's Pretend This Never Happened: A Mostly True Memoir, 阅读时间: 1356508800.0\n",
      "  新书籍ID: 131, 书名: The Winter Sea (Slains, #1), 阅读时间: 1369033200.0\n",
      "  新书籍ID: 76, 书名: To Kill a Mockingbird, 阅读时间: 1376228266.0\n",
      "  新书籍ID: 161, 书名: Bonhoeffer: Pastor, Martyr, Prophet, Spy, 阅读时间: 1377500400.0\n",
      "新用户ID: 4, 阅读记录数: 58\n",
      "  新书籍ID: 196, 书名: The Thorn Birds, 阅读时间: 662716800.0\n",
      "  新书籍ID: 197, 书名: Zorro, 阅读时间: 1193900400.0\n",
      "  新书籍ID: 32, 书名: The Secret History of the Pink Carnation (Pink Carnation, #1), 阅读时间: 1201852800.0\n",
      "  新书籍ID: 198, 书名: The English Patient, 阅读时间: 1206065565.0\n",
      "  新书籍ID: 199, 书名: The Shelters of Stone (Earth's Children, #5), 阅读时间: 1206255600.0\n",
      "\n",
      "调试信息：前5个用户ID映射\n",
      "旧用户ID: 792500e85277fa7ada535de23e7eb4c3, 新用户ID: 0\n",
      "旧用户ID: fc0a0792fd1c30427acdbfecbf5b0a20, 新用户ID: 1\n",
      "旧用户ID: ab2fadb5c7bbe55c80406d2b3692e969, 新用户ID: 2\n",
      "旧用户ID: 3b3f26019b3a5dbecb49c5faf1abce4c, 新用户ID: 3\n",
      "旧用户ID: d6804aa6e3a96b8e1104b8b9ac3fe882, 新用户ID: 4\n",
      "\n",
      "调试信息：前5个书籍ID映射\n",
      "旧书籍ID: 72929, 新书籍ID: 0, 书名: Affinity\n",
      "旧书籍ID: 6698199, 新书籍ID: 1, 书名: A Spy in the House (The Agency, #1)\n",
      "旧书籍ID: 7507889, 新书籍ID: 2, 书名: The Body at the Tower (The Agency, #2)\n",
      "旧书籍ID: 853180, 新书籍ID: 3, 书名: The Cater Street Hangman (Charlotte & Thomas Pitt, #1)\n",
      "旧书籍ID: 3870943, 新书籍ID: 4, 书名: The Apothecary's Daughter\n",
      "处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# # 读取数据\n",
    "# def load_data(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# # 重新映射用户ID和书籍ID\n",
    "# def remap_ids(data):\n",
    "#     user_id_map = {}\n",
    "#     book_id_map = {}\n",
    "#     new_user_id = 0\n",
    "#     new_book_id = 0\n",
    "    \n",
    "#     remapped_data = {}\n",
    "\n",
    "#     for user, books in data.items():\n",
    "#         # 如果用户ID没有映射过，分配新的ID\n",
    "#         if user not in user_id_map:\n",
    "#             user_id_map[user] = new_user_id\n",
    "#             new_user_id += 1\n",
    "        \n",
    "#         new_books = []\n",
    "#         for book in books:\n",
    "#             book_id = book['book_id']\n",
    "#             title = book['title']\n",
    "#             # 如果书籍ID没有映射过，分配新的ID\n",
    "#             if book_id not in book_id_map:\n",
    "#                 book_id_map[book_id] = (new_book_id, title)  # 保存book_id和title\n",
    "#                 new_book_id += 1\n",
    "            \n",
    "#             # 替换书籍ID\n",
    "#             new_books.append({\n",
    "#                 'book_id': book_id_map[book_id][0],  # 使用新ID\n",
    "#                 'title': title,\n",
    "#                 'read_at': book['read_at']\n",
    "#             })\n",
    "        \n",
    "#         # 使用新的用户ID和书籍ID\n",
    "#         remapped_data[user_id_map[user]] = new_books\n",
    "\n",
    "#     # 调试：打印前5个用户和书籍映射\n",
    "#     print(\"\\n调试信息：前5个用户和其书籍映射结果\")\n",
    "#     for i, (user, books) in enumerate(remapped_data.items()):\n",
    "#         if i >= 5:\n",
    "#             break\n",
    "#         print(f\"新用户ID: {user}, 阅读记录数: {len(books)}\")\n",
    "#         for book in books[:5]:  # 打印前5条阅读记录\n",
    "#             print(f\"  新书籍ID: {book['book_id']}, 书名: {book['title']}, 阅读时间: {book['read_at']}\")\n",
    "    \n",
    "#     return remapped_data, user_id_map, book_id_map\n",
    "\n",
    "# # 生成 id2name.txt 文件\n",
    "# def generate_id2name(book_id_map, output_file):\n",
    "#     # 按书籍ID排序\n",
    "#     sorted_books = sorted(book_id_map.items(), key=lambda x: x[1][0])  # 按照新的ID排序\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for book_id, (new_id, title) in sorted_books:\n",
    "#             f.write(f\"{new_id}:: {title}\\n\")\n",
    "\n",
    "# # 主流程\n",
    "# file_path = '/workspace/goodreads/processed_ba_user_sessions.json'\n",
    "# data = load_data(file_path)\n",
    "\n",
    "# # 重新映射用户ID和书籍ID\n",
    "# remapped_data, user_id_map, book_id_map = remap_ids(data)\n",
    "\n",
    "# # 将处理后的数据保存回 JSON 文件\n",
    "# with open('/workspace/goodreads/remapped_ya_user_sessions.json', 'w') as f:\n",
    "#     json.dump(remapped_data, f, indent=2)\n",
    "\n",
    "# # 生成 id2name.txt 文件\n",
    "# generate_id2name(book_id_map, '/workspace/goodreads/id2name.txt')\n",
    "\n",
    "# # 保存用户ID映射表\n",
    "# with open('/workspace/goodreads/user_id_map.json', 'w') as f:\n",
    "#     json.dump(user_id_map, f, indent=2)\n",
    "\n",
    "# # 调试：检查映射表\n",
    "# print(\"\\n调试信息：前5个用户ID映射\")\n",
    "# for i, (old_id, new_id) in enumerate(user_id_map.items()):\n",
    "#     if i >= 5:\n",
    "#         break\n",
    "#     print(f\"旧用户ID: {old_id}, 新用户ID: {new_id}\")\n",
    "\n",
    "# print(\"\\n调试信息：前5个书籍ID映射\")\n",
    "# for i, (old_id, (new_id, title)) in enumerate(book_id_map.items()):\n",
    "#     if i >= 5:\n",
    "#         break\n",
    "#     print(f\"旧书籍ID: {old_id}, 新书籍ID: {new_id}, 书名: {title}\")\n",
    "\n",
    "# print(\"处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "677d106f-9da9-47ae-99bb-cf04f75e3d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集生成完成，总记录数: 72697\n",
      "前5条训练集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [1666, 895, 728, 283, 1799, 263, 1237, 1111, 5...       10  1394\n",
      "1  [375, 1362, 2009, 561, 1560, 1595, 1378, 1663,...       10   239\n",
      "2  [1484, 1318, 319, 736, 704, 99, 1019, 913, 136...       10  1988\n",
      "3  [979, 25, 29, 1739, 2359, 2359, 2359, 2359, 23...        4   658\n",
      "4  [1049, 1172, 907, 1061, 1594, 1033, 151, 489, ...       10   172\n",
      "验证/测试集生成完成，总记录数: 112\n",
      "前5条验证/测试集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [184, 135, 1432, 1475, 1120, 478, 1286, 193, 2...       10  1434\n",
      "1  [932, 2056, 2101, 1479, 1643, 1477, 93, 1668, ...       10    73\n",
      "2  [59, 224, 32, 643, 1851, 506, 174, 518, 807, 519]       10  1801\n",
      "3  [2172, 799, 800, 806, 1130, 808, 1784, 1185, 1...       10    83\n",
      "4  [475, 128, 530, 59, 1135, 607, 454, 951, 136, ...       10   224\n",
      "验证/测试集生成完成，总记录数: 112\n",
      "前5条验证/测试集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [2137, 208, 573, 10, 1159, 1839, 1099, 1610, 5...       10   544\n",
      "1  [385, 942, 1260, 130, 606, 605, 961, 1356, 227...       10  2053\n",
      "2  [2087, 1701, 322, 1784, 1473, 1708, 1523, 1477...       10  1481\n",
      "3  [708, 1471, 1210, 1318, 1345, 2221, 1479, 1473...       10  1708\n",
      "4  [2074, 550, 1564, 310, 916, 313, 778, 639, 129...       10   478\n",
      "训练集 集合长度: 72697\n",
      "训练集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [1666, 895, 728, 283, 1799, 263, 1237, 1111, 5...       10  1394\n",
      "1  [375, 1362, 2009, 561, 1560, 1595, 1378, 1663,...       10   239\n",
      "2  [1484, 1318, 319, 736, 704, 99, 1019, 913, 136...       10  1988\n",
      "训练集 中 len_seq 的最小值: 1\n",
      "验证集 集合长度: 112\n",
      "验证集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [184, 135, 1432, 1475, 1120, 478, 1286, 193, 2...       10  1434\n",
      "1  [932, 2056, 2101, 1479, 1643, 1477, 93, 1668, ...       10    73\n",
      "2  [59, 224, 32, 643, 1851, 506, 174, 518, 807, 519]       10  1801\n",
      "验证集 中 len_seq 的最小值: 10\n",
      "测试集 集合长度: 112\n",
      "测试集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [2137, 208, 573, 10, 1159, 1839, 1099, 1610, 5...       10   544\n",
      "1  [385, 942, 1260, 130, 606, 605, 961, 1356, 227...       10  2053\n",
      "2  [2087, 1701, 322, 1784, 1473, 1708, 1523, 1477...       10  1481\n",
      "测试集 中 len_seq 的最小值: 10\n",
      "数据集生成并保存完成。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 填充历史序列的函数，将 pad_item 填充到序列末尾\n",
    "def pad_history(itemlist, length, pad_item):\n",
    "    if len(itemlist) >= length:\n",
    "        return itemlist[-length:]\n",
    "    else:\n",
    "        return itemlist + [pad_item] * (length - len(itemlist))\n",
    "\n",
    "# 生成训练集的函数，并过滤掉 len_seq = 0 的记录\n",
    "def generate_train_sequences(data, length=10, pad_item=2359):\n",
    "    state, len_state, action = [], [], []\n",
    "    \n",
    "    for user_id, books in data.items():\n",
    "        history = []\n",
    "        for index, book in enumerate(books):\n",
    "            s = list(history)  # 复制当前的历史记录\n",
    "            if len(history) > 0:  # 只生成有效的历史序列\n",
    "                len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "                s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "                state.append(s)\n",
    "                action.append(book['book_id'])  # 预测的下一本书\n",
    "\n",
    "            # 更新历史记录\n",
    "            history.append(book['book_id'])\n",
    "    \n",
    "    # 创建 DataFrame 并确保索引从 0 开始\n",
    "    train_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 打印一些调试信息\n",
    "    print(f\"训练集生成完成，总记录数: {len(train_df)}\")\n",
    "    print(f\"前5条训练集记录: \\n{train_df.head()}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# # 生成训练集的函数，并过滤掉 len_seq = 0 的记录，且打乱顺序\n",
    "# def generate_train_sequences(data, length=10, pad_item=2359):\n",
    "#     state, len_state, action = [], [], []\n",
    "    \n",
    "#     for user_id, books in data.items():\n",
    "#         history = []\n",
    "#         for index, book in enumerate(books):\n",
    "#             s = list(history)  # 复制当前的历史记录\n",
    "#             if len(history) > 0:  # 只生成有效的历史序列\n",
    "#                 len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "#                 s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "#                 state.append(s)\n",
    "#                 action.append(book['book_id'])  # 预测的下一本书\n",
    "\n",
    "#             # 更新历史记录\n",
    "#             history.append(book['book_id'])\n",
    "    \n",
    "#     # 创建 DataFrame 并确保索引从 0 开始\n",
    "#     train_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "#     train_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     # 打乱数据\n",
    "#     train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#     # 打印一些调试信息\n",
    "#     print(f\"训练集生成完成，总记录数: {len(train_df)}\")\n",
    "#     print(f\"前5条训练集记录: \\n{train_df.head()}\")\n",
    "\n",
    "#     return train_df\n",
    "\n",
    "# 生成验证集和测试集的函数，并过滤掉 len_seq = 0 的记录\n",
    "def generate_test_sequences(data, length=10, pad_item=2359):\n",
    "    state, len_state, action = [], [], []\n",
    "    \n",
    "    for user_id, books in data.items():\n",
    "        history = [book['book_id'] for book in books]\n",
    "        \n",
    "        if len(history) > 1:\n",
    "            s = history[:-1]  # 最后一条作为预测目标，之前的作为历史记录\n",
    "        else:\n",
    "            s = []\n",
    "\n",
    "        if len(s) > 0:  # 只生成有效的历史序列\n",
    "            len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "            s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "            state.append(s)\n",
    "            action.append(history[-1])  # 最后一条作为预测目标\n",
    "    \n",
    "    # 创建 DataFrame 并确保索引从 0 开始\n",
    "    test_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 打印一些调试信息\n",
    "    print(f\"验证/测试集生成完成，总记录数: {len(test_df)}\")\n",
    "    print(f\"前5条验证/测试集记录: \\n{test_df.head()}\")\n",
    "\n",
    "    return test_df\n",
    "\n",
    "# 检查数据集\n",
    "def check_data(df, name):\n",
    "    print(f\"{name} 集合长度: {len(df)}\")\n",
    "    print(f\"{name} 集合中前 3 条记录:\\n{df.head(3)}\")\n",
    "    print(f\"{name} 中 len_seq 的最小值: {df['len_seq'].min()}\")\n",
    "    assert df['len_seq'].min() > 0, f\"{name} 集合中有 len_seq = 0 的记录！\"\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/history/remapped_ya_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "\n",
    "# 假设用户按 8:1:1 划分为训练、验证和测试集\n",
    "total_users = list(data.keys())\n",
    "fractions = [0.8, 0.1, 0.1]\n",
    "train_users, val_users, test_users = np.split(total_users, [int(0.8*len(total_users)), int(0.9*len(total_users))])\n",
    "\n",
    "# 根据用户划分生成训练集、验证集和测试集\n",
    "train_data = {user: data[user] for user in train_users}\n",
    "val_data = {user: data[user] for user in val_users}\n",
    "test_data = {user: data[user] for user in test_users}\n",
    "\n",
    "# 生成并保存训练集\n",
    "train_df = generate_train_sequences(train_data)\n",
    "train_df.to_pickle('/workspace/history/train_data.df')\n",
    "\n",
    "# 生成并保存验证集\n",
    "val_df = generate_test_sequences(val_data)\n",
    "val_df.to_pickle('/workspace/history/val_data.df')\n",
    "\n",
    "# 生成并保存测试集\n",
    "test_df = generate_test_sequences(test_data)\n",
    "test_df.to_pickle('/workspace/history/test_data.df')\n",
    "\n",
    "# 检查生成的 DataFrame\n",
    "check_data(train_df, \"训练集\")\n",
    "check_data(val_df, \"验证集\")\n",
    "check_data(test_df, \"测试集\")\n",
    "\n",
    "print(\"数据集生成并保存完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9323f4bf-bc8f-4c23-8b6b-6d6c1c1daab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗完成，已保存到 /workspace/history/id2name_clean.txt\n",
      "清洗后条目数: 2359\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_id2name_remove_brackets(file_path, output_path):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    # 定义一个正则表达式，匹配括号及其内部的内容\n",
    "    pattern = r'\\s*\\(.*?\\)'\n",
    "\n",
    "    # 读取并清洗数据\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('::')\n",
    "            if len(parts) == 2:  # 确保有 id 和 book_name 两部分\n",
    "                book_id, book_name = parts\n",
    "                if book_name:  # 确保书名不为空\n",
    "                    # 使用正则表达式去掉括号和其中的内容\n",
    "                    book_name_cleaned = re.sub(pattern, '', book_name).strip()\n",
    "                    cleaned_data.append(f\"{book_id}::{book_name_cleaned}\")\n",
    "    \n",
    "    # 保存清洗后的数据\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for line in cleaned_data:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    print(f\"清洗完成，已保存到 {output_path}\")\n",
    "    print(f\"清洗后条目数: {len(cleaned_data)}\")\n",
    "\n",
    "# 调用函数进行清洗\n",
    "file_path = '/workspace/history/id2name.txt'\n",
    "output_path = '/workspace/history/id2name_clean.txt'\n",
    "\n",
    "clean_id2name_remove_brackets(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cae3034-e542-4a2b-978d-674335343254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0::Affinity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1::Elizabeth Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2::A Spy in the House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3::The Body at the Tower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4::The Traitor in the Tunnel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>2354::Nikola Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>2355::When Nietzsche Wept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>2356::This Blinding Absence of Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>2357::Assata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>2358::Life Without Limits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2359 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Cleaned Titles\n",
       "0                              0::Affinity\n",
       "1                      1::Elizabeth Street\n",
       "2                    2::A Spy in the House\n",
       "3                 3::The Body at the Tower\n",
       "4             4::The Traitor in the Tunnel\n",
       "...                                    ...\n",
       "2354                    2354::Nikola Tesla\n",
       "2355             2355::When Nietzsche Wept\n",
       "2356  2356::This Blinding Absence of Light\n",
       "2357                          2357::Assata\n",
       "2358             2358::Life Without Limits\n",
       "\n",
       "[2359 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = '/workspace/history/id2name_clean.txt'\n",
    "\n",
    "# 读取文件内容\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    titles = f.readlines()\n",
    "\n",
    "# 去除每行末尾的换行符\n",
    "titles = [title.strip() for title in titles]\n",
    "\n",
    "# 定义清洗标题的函数，去除第一个冒号后的内容\n",
    "def clean_title(title):\n",
    "    # 分割 '::' 以区分ID和标题\n",
    "    parts = title.split(\"::\")\n",
    "    book_id = parts[0]\n",
    "    # 清除标题中的多余部分\n",
    "    cleaned_title = re.sub(r':.*', '', parts[1]).strip()\n",
    "    return f\"{book_id}::{cleaned_title}\"\n",
    "\n",
    "# 清洗所有标题\n",
    "cleaned_titles = [clean_title(title) for title in titles]\n",
    "\n",
    "# 转换为 DataFrame\n",
    "cleaned_titles_df = pd.DataFrame(cleaned_titles, columns=[\"Cleaned Titles\"])\n",
    "\n",
    "# 保存清洗后的结果到新文件\n",
    "output_file_path = '/workspace/history/id2name.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(cleaned_titles))\n",
    "\n",
    "# 显示清洗后的 DataFrame\n",
    "cleaned_titles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f0e3c-4b7c-4812-9a51-7502e3210ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
