{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188c9aea-7441-438b-930b-493af291704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据中...\n",
      "分析数据中...\n",
      "总评论数: 2389900\n",
      "总用户数: 209152\n",
      "总书籍数: 93267\n",
      "\n",
      "用户评论数分析:\n",
      "平均每个用户的评论数: 11.43\n",
      "最少评论数: 1, 最多评论数: 2438\n",
      "\n",
      "书籍评论数分析:\n",
      "平均每本书的评论数: 25.62\n",
      "最少评论数: 1, 最多评论数: 20756\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取 Goodreads 子类数据\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# 统计基本信息\n",
    "def analyze_reviews(reviews):\n",
    "    user_count = defaultdict(int)\n",
    "    book_count = defaultdict(int)\n",
    "    \n",
    "    # 统计每个用户的评论次数和每本书的评论次数\n",
    "    for review in reviews:\n",
    "        user_id = review['user_id']\n",
    "        book_id = review['book_id']\n",
    "        user_count[user_id] += 1\n",
    "        book_count[book_id] += 1\n",
    "    \n",
    "    total_reviews = len(reviews)\n",
    "    total_users = len(user_count)\n",
    "    total_books = len(book_count)\n",
    "    \n",
    "    print(f\"总评论数: {total_reviews}\")\n",
    "    print(f\"总用户数: {total_users}\")\n",
    "    print(f\"总书籍数: {total_books}\")\n",
    "    \n",
    "    # 分析评论次数分布\n",
    "    user_review_counts = list(user_count.values())\n",
    "    book_review_counts = list(book_count.values())\n",
    "    \n",
    "    print(f\"\\n用户评论数分析:\")\n",
    "    print(f\"平均每个用户的评论数: {sum(user_review_counts) / len(user_review_counts):.2f}\")\n",
    "    print(f\"最少评论数: {min(user_review_counts)}, 最多评论数: {max(user_review_counts)}\")\n",
    "    \n",
    "    print(f\"\\n书籍评论数分析:\")\n",
    "    print(f\"平均每本书的评论数: {sum(book_review_counts) / len(book_review_counts):.2f}\")\n",
    "    print(f\"最少评论数: {min(book_review_counts)}, 最多评论数: {max(book_review_counts)}\")\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "reviews = load_reviews(file_path)\n",
    "\n",
    "print(\"分析数据中...\")\n",
    "analyze_reviews(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bcf74b9-8d38-4c32-beff-19c9b57e0b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印第一个 JSON 对象中的特征:\n",
      "{\n",
      "  \"user_id\": \"8842281e1d1347389f2ab93d60773d4d\",\n",
      "  \"book_id\": \"2767052\",\n",
      "  \"review_id\": \"248c011811e945eca861b5c31a549291\",\n",
      "  \"rating\": 5,\n",
      "  \"review_text\": \"I cracked and finally picked this up. Very enjoyable quick read - couldn't put it down - it was like crack. \\n I'm a bit bothered by the lack of backstory of how Panem and the Hunger Games come about. It is just kind of explained away in a few paragraphs and we are left to accept this very strange world where teenagers are pitted into an arena each year to kill each other? I was expecting it because I've seen Battle Royale, but I would have appreciated knowing more of the backstory of how the world could have come into such a odd state. \\n I suppose what makes a book like this interesting is thinking about the strategy of it all. The players are going to be statistically encouraged to band together because they will last longer that way, but by definition of course any partnership will be broken, and the drama of how that unfolds is always interesting and full of friendships broken and betrayal. Each character approached the game in their own way. Some banded together in larger coalitions, some were loners initially and banded together later. And some were just loners, like Foxface. A lot depended on your survival skill: could you find food and water on your own? Self-dependence is highly valued - and of course our hero was strong there. \\n All in all, a fun read, but I feel kind of dirty for having read it.\",\n",
      "  \"date_added\": \"Wed Jan 13 13:38:25 -0800 2010\",\n",
      "  \"date_updated\": \"Wed Mar 22 11:46:36 -0700 2017\",\n",
      "  \"read_at\": \"Sun Mar 25 00:00:00 -0700 2012\",\n",
      "  \"started_at\": \"Fri Mar 23 00:00:00 -0700 2012\",\n",
      "  \"n_votes\": 24,\n",
      "  \"n_comments\": 25\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "# 读取 Goodreads 子类数据的一个 JSON 对象\n",
    "def print_first_review(file_path):\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        first_line = f.readline()  # 读取第一行数据\n",
    "        first_review = json.loads(first_line)  # 将其转为 JSON 格式\n",
    "        print(json.dumps(first_review, indent=2))  # 格式化输出 JSON 对象\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "\n",
    "# 打印第一个 JSON 对象\n",
    "print(\"打印第一个 JSON 对象中的特征:\")\n",
    "print_first_review(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88a964b-ea42-4288-a48a-bc5b20a92d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据中...\n",
      "最受欢迎的 4000 本书的评论次数分布:\n",
      "书籍ID: 11870085, 评论次数: 20756\n",
      "书籍ID: 2767052, 评论次数: 18617\n",
      "书籍ID: 7260188, 评论次数: 13536\n",
      "书籍ID: 6148028, 评论次数: 11904\n",
      "书籍ID: 13335037, 评论次数: 10743\n",
      "书籍ID: 41865, 评论次数: 10535\n",
      "书籍ID: 15745753, 评论次数: 9590\n",
      "书籍ID: 11235712, 评论次数: 9585\n",
      "书籍ID: 9460487, 评论次数: 9557\n",
      "书籍ID: 11735983, 评论次数: 9207\n",
      "\n",
      "用户在前4000本书中的阅读情况:\n",
      "用户ID: 8842281e1d1347389f2ab93d60773d4d, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 7504b2aee1ecb5b2872d3da381c6c91e, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: f8a89075dc6de14857561522e729f82c, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: 704eb93a316aff687a93d5215882eb21, 阅读涉及的4000本书中的数量: 3\n",
      "用户ID: 012515e5802b2e0f42915118c90fa04b, 阅读涉及的4000本书中的数量: 31\n",
      "用户ID: f4d16ea4ac59af59d257631398af39f4, 阅读涉及的4000本书中的数量: 3\n",
      "用户ID: 01ec1a320ffded6b2dd47833f2c8e4fb, 阅读涉及的4000本书中的数量: 19\n",
      "用户ID: 4b3636a043e5c99fa27ac897ccfa1151, 阅读涉及的4000本书中的数量: 8\n",
      "用户ID: 903d4b859e86a1dd6d7640849cc7067c, 阅读涉及的4000本书中的数量: 1\n",
      "用户ID: afc070543f19028dc7e7f084a0079f72, 阅读涉及的4000本书中的数量: 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# 读取 Goodreads 子类数据\n",
    "def load_reviews(file_path):\n",
    "    reviews = []\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "# 选出最受欢迎的前4000本书\n",
    "def select_top_books(reviews, top_n=4000):\n",
    "    book_count = defaultdict(int)\n",
    "    \n",
    "    # 统计每本书的评论次数\n",
    "    for review in reviews:\n",
    "        book_id = review['book_id']\n",
    "        book_count[book_id] += 1\n",
    "    \n",
    "    # 按评论次数排序，选出前 top_n 本书\n",
    "    top_books = sorted(book_count.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_book_ids = set([book_id for book_id, count in top_books])\n",
    "    \n",
    "    return top_books, top_book_ids\n",
    "\n",
    "# 统计每个用户在前4000本书中的阅读情况\n",
    "def analyze_user_reading(reviews, top_book_ids):\n",
    "    user_top_book_count = defaultdict(int)\n",
    "    \n",
    "    # 按用户统计其阅读的前4000本书的数量\n",
    "    for review in reviews:\n",
    "        user_id = review['user_id']\n",
    "        book_id = review['book_id']\n",
    "        if book_id in top_book_ids:\n",
    "            user_top_book_count[user_id] += 1\n",
    "    \n",
    "    return user_top_book_count\n",
    "\n",
    "# 主流程\n",
    "file_path = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "\n",
    "print(\"加载数据中...\")\n",
    "reviews = load_reviews(file_path)\n",
    "\n",
    "# 选出最受欢迎的前4000本书\n",
    "top_books, top_book_ids = select_top_books(reviews, top_n=4000)\n",
    "\n",
    "# 输出最受欢迎的书籍及其评论次数\n",
    "print(f\"最受欢迎的 4000 本书的评论次数分布:\")\n",
    "for book_id, count in top_books[:10]:  # 仅输出前10本书的评论数\n",
    "    print(f\"书籍ID: {book_id}, 评论次数: {count}\")\n",
    "\n",
    "# 统计每个用户在这 4000 本书中阅读的数量\n",
    "user_top_book_count = analyze_user_reading(reviews, top_book_ids)\n",
    "\n",
    "# 输出部分用户阅读情况\n",
    "print(f\"\\n用户在前4000本书中的阅读情况:\")\n",
    "for user_id, count in list(user_top_book_count.items())[:10]:  # 仅输出前10个用户\n",
    "    print(f\"用户ID: {user_id}, 阅读涉及的4000本书中的数量: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a9fa99-fe1f-47f2-ab84-e1101c16046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读最多书的用户ID: aca760854b57ce2ec981df32e46dc96c, 阅读的4000本热门书中的数量: 1126\n",
      "读最少书的用户ID: 7b4166acdefeb4adb7d12d9c1645c48f, 阅读的4000本热门书中的数量: 142\n",
      "\n",
      "前 1000 用户在4000本热门书中的阅读情况:\n",
      "用户ID: aca760854b57ce2ec981df32e46dc96c, 阅读涉及的4000本书中的数量: 1126\n",
      "用户ID: 288dc8c9871098c8a1b680db829275b4, 阅读涉及的4000本书中的数量: 817\n",
      "用户ID: aed35dbc626957174ebedf3c555b63d0, 阅读涉及的4000本书中的数量: 770\n",
      "用户ID: d321b1bcf294bca33510816afd898eb3, 阅读涉及的4000本书中的数量: 748\n",
      "用户ID: af9864c9e69963abb963fe2c90dd6f09, 阅读涉及的4000本书中的数量: 739\n",
      "用户ID: 63eb5a9ea6fbce905e96dadf97e60c93, 阅读涉及的4000本书中的数量: 685\n",
      "用户ID: 19ff136f47089904d689e69e36c991d0, 阅读涉及的4000本书中的数量: 684\n",
      "用户ID: 667b94d4c7e0b014bb6ab3636999e712, 阅读涉及的4000本书中的数量: 658\n",
      "用户ID: 0d344a261de9ab42e62cf9b3b7c52cc4, 阅读涉及的4000本书中的数量: 649\n",
      "用户ID: 884719ebf7dbd2977768e179358f6758, 阅读涉及的4000本书中的数量: 646\n"
     ]
    }
   ],
   "source": [
    "# 按照用户阅读的热门书籍数量排序，并选出前 1000 个用户\n",
    "def get_top_n_users(user_top_book_count, top_n=1000):\n",
    "    sorted_users = sorted(user_top_book_count.items(), key=lambda x: x[1], reverse=True)  # 按照阅读量从大到小排序\n",
    "    top_users = sorted_users[:top_n]  # 选出前 top_n 个用户\n",
    "    return top_users\n",
    "\n",
    "# 显示读最多和读最少的用户的阅读量\n",
    "def display_min_max_user_reading(top_users):\n",
    "    max_read_user = top_users[0]\n",
    "    min_read_user = top_users[-1]\n",
    "    \n",
    "    print(f\"读最多书的用户ID: {max_read_user[0]}, 阅读的4000本热门书中的数量: {max_read_user[1]}\")\n",
    "    print(f\"读最少书的用户ID: {min_read_user[0]}, 阅读的4000本热门书中的数量: {min_read_user[1]}\")\n",
    "\n",
    "# 筛选出前 1000 个用户\n",
    "top_n = 1000\n",
    "top_users = get_top_n_users(user_top_book_count, top_n=top_n)\n",
    "\n",
    "# 显示读最多书和读最少书的用户的阅读量\n",
    "display_min_max_user_reading(top_users)\n",
    "\n",
    "# 输出前1000个用户的阅读情况\n",
    "print(f\"\\n前 {top_n} 用户在4000本热门书中的阅读情况:\")\n",
    "for user_id, count in top_users[:10]:  # 仅显示前10个用户的阅读情况\n",
    "    print(f\"用户ID: {user_id}, 阅读涉及的4000本书中的数量: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e6a9537-6320-4adf-8d8d-872abe6dcae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying top 4000 books...\n",
      "Analyzing user reading patterns...\n",
      "Selecting top 1000 users...\n",
      "Processing and saving data...\n",
      "\n",
      "Top 4000 books:\n",
      "Most reviewed book: 20756 reviews\n",
      "Least reviewed book among top 4000: 82 reviews\n",
      "\n",
      "Top 1000 users:\n",
      "User with most books read: 1126 books\n",
      "User with least books read among top 1000: 142 books\n",
      "\n",
      "Processed data saved to processed_ya_user_sessions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import heapq\n",
    "\n",
    "def parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_string, \"%a %b %d %H:%M:%S %z %Y\").timestamp()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def get_top_books(file_path, top_n=4000):\n",
    "    book_counts = Counter()\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            book_counts[review['book_id']] += 1\n",
    "    return dict(book_counts.most_common(top_n))\n",
    "\n",
    "def get_user_reading_counts(file_path, top_books):\n",
    "    user_reading_counts = defaultdict(int)\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            if review['book_id'] in top_books:\n",
    "                user_reading_counts[review['user_id']] += 1\n",
    "    return user_reading_counts\n",
    "\n",
    "def get_top_users(user_reading_counts, top_n=1000):\n",
    "    return dict(sorted(user_reading_counts.items(), key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "def load_book_titles(file_path):\n",
    "    book_titles = {}\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        for line in f:\n",
    "            book = json.loads(line)\n",
    "            book_titles[book['book_id']] = book['title']\n",
    "    return book_titles\n",
    "\n",
    "def process_and_save_data(reviews_file, books_file, output_file, top_books, top_users):\n",
    "    book_titles = load_book_titles(books_file)\n",
    "    user_data = defaultdict(list)\n",
    "    \n",
    "    with gzip.open(reviews_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            review = json.loads(line)\n",
    "            user_id = review['user_id']\n",
    "            book_id = review['book_id']\n",
    "            \n",
    "            if user_id in top_users and book_id in top_books:\n",
    "                read_at = parse_date(review.get('read_at')) or parse_date(review.get('date_added'))\n",
    "                title = book_titles.get(book_id, \"Unknown Title\")\n",
    "                user_data[user_id].append({\n",
    "                    'book_id': book_id,\n",
    "                    'title': title,\n",
    "                    'read_at': read_at\n",
    "                })\n",
    "    \n",
    "    # Sort each user's reading list by date\n",
    "    for user_id, books in user_data.items():\n",
    "        user_data[user_id] = sorted(books, key=lambda x: x['read_at'] or 0)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(user_data, f)\n",
    "\n",
    "# Main processing\n",
    "reviews_file = '/workspace/goodreads/goodreads_reviews_young_adult.json.gz'\n",
    "books_file = '/workspace/goodreads/goodreads_books.json.gz'\n",
    "output_file = 'processed_ya_user_sessions.json'\n",
    "\n",
    "print(\"Identifying top 4000 books...\")\n",
    "top_books = get_top_books(reviews_file)\n",
    "\n",
    "print(\"Analyzing user reading patterns...\")\n",
    "user_reading_counts = get_user_reading_counts(reviews_file, top_books)\n",
    "\n",
    "print(\"Selecting top 1000 users...\")\n",
    "top_users = get_top_users(user_reading_counts)\n",
    "\n",
    "print(\"Processing and saving data...\")\n",
    "process_and_save_data(reviews_file, books_file, output_file, top_books, top_users)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTop 4000 books:\")\n",
    "print(f\"Most reviewed book: {max(top_books.values())} reviews\")\n",
    "print(f\"Least reviewed book among top 4000: {min(top_books.values())} reviews\")\n",
    "\n",
    "print(f\"\\nTop 1000 users:\")\n",
    "print(f\"User with most books read: {max(top_users.values())} books\")\n",
    "print(f\"User with least books read among top 1000: {min(top_users.values())} books\")\n",
    "\n",
    "print(f\"\\nProcessed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0f43f4e-0b3f-44c7-b97e-8e1f6bae8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数: 1000\n",
      "平均阅读量: 220.56\n",
      "中位数阅读量: 188\n",
      "最小阅读量: 142\n",
      "最大阅读量: 1126\n",
      "\n",
      "平均阅读时间跨度: 2607.09 天\n",
      "\n",
      "最受欢迎的10本书:\n",
      "Cinder (The Lunar Chronicles, #1): 661 次阅读\n",
      "Divergent (Divergent, #1): 654 次阅读\n",
      "The Fault in Our Stars: 603 次阅读\n",
      "The Raven Boys (The Raven Cycle, #1): 558 次阅读\n",
      "Throne of Glass (Throne of Glass, #1): 556 次阅读\n",
      "Scarlet (The Lunar Chronicles, #2): 546 次阅读\n",
      "Insurgent (Divergent, #2): 535 次阅读\n",
      "Anna and the French Kiss (Anna and the French Kiss, #1): 531 次阅读\n",
      "Fangirl: 525 次阅读\n",
      "Shadow and Bone (The Grisha, #1): 523 次阅读\n",
      "\n",
      "随机用户 711bafffa3a80f5829c55a47360c7864 的前5条阅读记录:\n",
      "书名: Hexbound (Dark Elite, #2), 阅读时间: 2011-05-04 07:00:00\n",
      "书名: The DUFF: Designated Ugly Fat Friend, 阅读时间: 2011-05-10 07:00:00\n",
      "书名: The Summer I Turned Pretty (Summer, #1), 阅读时间: 2011-05-12 07:00:00\n",
      "书名: Possess, 阅读时间: 2011-11-04 07:00:00\n",
      "书名: Silence (Hush, Hush, #3), 阅读时间: 2011-11-17 08:00:00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_data(data):\n",
    "    user_book_counts = {user: len(books) for user, books in data.items()}\n",
    "    reading_spans = {}\n",
    "    books_counter = Counter()\n",
    "\n",
    "    for user, books in data.items():\n",
    "        if books:\n",
    "            dates = [book['read_at'] for book in books if book['read_at']]\n",
    "            if dates:\n",
    "                reading_spans[user] = max(dates) - min(dates)\n",
    "        books_counter.update([book['title'] for book in books])\n",
    "\n",
    "    return user_book_counts, reading_spans, books_counter\n",
    "\n",
    "def print_statistics(user_book_counts, reading_spans, books_counter):\n",
    "    print(f\"用户数: {len(user_book_counts)}\")\n",
    "    print(f\"平均阅读量: {sum(user_book_counts.values()) / len(user_book_counts):.2f}\")\n",
    "    print(f\"中位数阅读量: {sorted(user_book_counts.values())[len(user_book_counts)//2]}\")\n",
    "    print(f\"最小阅读量: {min(user_book_counts.values())}\")\n",
    "    print(f\"最大阅读量: {max(user_book_counts.values())}\")\n",
    "\n",
    "    avg_span = sum(reading_spans.values()) / len(reading_spans) / (24 * 3600)  # 转换为天\n",
    "    print(f\"\\n平均阅读时间跨度: {avg_span:.2f} 天\")\n",
    "\n",
    "    print(\"\\n最受欢迎的10本书:\")\n",
    "    for book, count in books_counter.most_common(10):\n",
    "        print(f\"{book}: {count} 次阅读\")\n",
    "\n",
    "    # 随机选择一个用户进行样本检查\n",
    "    sample_user = random.choice(list(data.keys()))\n",
    "    print(f\"\\n随机用户 {sample_user} 的前5条阅读记录:\")\n",
    "    for book in data[sample_user][:5]:\n",
    "        print(f\"书名: {book['title']}, 阅读时间: {datetime.fromtimestamp(book['read_at'])}\")\n",
    "\n",
    "# 主处理流程\n",
    "file_path = 'processed_ya_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "user_book_counts, reading_spans, books_counter = analyze_data(data)\n",
    "print_statistics(user_book_counts, reading_spans, books_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f81d3524-2f69-4bc8-8fe5-53d70c4c73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数: 1000\n",
      "平均阅读量: 220.56\n",
      "中位数阅读量: 188\n",
      "最小阅读量: 142\n",
      "最大阅读量: 1126\n",
      "\n",
      "平均阅读时间跨度: 2607.09 天\n",
      "\n",
      "unique book 总数: 3934\n",
      "\n",
      "最受欢迎的10本书:\n",
      "Cinder (The Lunar Chronicles, #1): 661 次阅读\n",
      "Divergent (Divergent, #1): 654 次阅读\n",
      "The Fault in Our Stars: 603 次阅读\n",
      "The Raven Boys (The Raven Cycle, #1): 558 次阅读\n",
      "Throne of Glass (Throne of Glass, #1): 556 次阅读\n",
      "Scarlet (The Lunar Chronicles, #2): 546 次阅读\n",
      "Insurgent (Divergent, #2): 535 次阅读\n",
      "Anna and the French Kiss (Anna and the French Kiss, #1): 531 次阅读\n",
      "Fangirl: 525 次阅读\n",
      "Shadow and Bone (The Grisha, #1): 523 次阅读\n",
      "\n",
      "随机用户 ffa5094acb2bca8fc8655538e60c400e 的前5条阅读记录:\n",
      "书名: Dreamhunter (The Dreamhunter Duet, #1), 阅读时间: 2009-01-01 08:00:00\n",
      "书名: Magic Under Glass (Magic Under, #1), 阅读时间: 2010-05-01 07:00:00\n",
      "书名: Matched (Matched, #1), 阅读时间: 2010-07-25 07:00:00\n",
      "书名: Clockwork Angel (The Infernal Devices, #1), 阅读时间: 2010-07-28 07:00:00\n",
      "书名: The Eternal Ones (Eternal Ones, #1), 阅读时间: 2010-07-31 07:00:00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_data(data):\n",
    "    user_book_counts = {user: len(books) for user, books in data.items()}\n",
    "    reading_spans = {}\n",
    "    books_counter = Counter()\n",
    "    unique_books = set()\n",
    "\n",
    "    for user, books in data.items():\n",
    "        if books:\n",
    "            dates = [book['read_at'] for book in books if book['read_at']]\n",
    "            if dates:\n",
    "                reading_spans[user] = max(dates) - min(dates)\n",
    "        \n",
    "        for book in books:\n",
    "            books_counter.update([book['title']])\n",
    "            unique_books.add(book['book_id'])  # 将 book_id 加入唯一书籍集合\n",
    "\n",
    "    return user_book_counts, reading_spans, books_counter, unique_books\n",
    "\n",
    "def print_statistics(user_book_counts, reading_spans, books_counter, unique_books):\n",
    "    print(f\"用户数: {len(user_book_counts)}\")\n",
    "    print(f\"平均阅读量: {sum(user_book_counts.values()) / len(user_book_counts):.2f}\")\n",
    "    print(f\"中位数阅读量: {sorted(user_book_counts.values())[len(user_book_counts)//2]}\")\n",
    "    print(f\"最小阅读量: {min(user_book_counts.values())}\")\n",
    "    print(f\"最大阅读量: {max(user_book_counts.values())}\")\n",
    "\n",
    "    avg_span = sum(reading_spans.values()) / len(reading_spans) / (24 * 3600)  # 转换为天\n",
    "    print(f\"\\n平均阅读时间跨度: {avg_span:.2f} 天\")\n",
    "\n",
    "    print(f\"\\nunique book 总数: {len(unique_books)}\")  # 打印 unique book 数量\n",
    "\n",
    "    print(\"\\n最受欢迎的10本书:\")\n",
    "    for book, count in books_counter.most_common(10):\n",
    "        print(f\"{book}: {count} 次阅读\")\n",
    "\n",
    "    # 随机选择一个用户进行样本检查\n",
    "    sample_user = random.choice(list(data.keys()))\n",
    "    print(f\"\\n随机用户 {sample_user} 的前5条阅读记录:\")\n",
    "    for book in data[sample_user][:5]:\n",
    "        print(f\"书名: {book['title']}, 阅读时间: {datetime.fromtimestamp(book['read_at'])}\")\n",
    "\n",
    "# 主处理流程\n",
    "file_path = 'processed_ya_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "user_book_counts, reading_spans, books_counter, unique_books = analyze_data(data)\n",
    "print_statistics(user_book_counts, reading_spans, books_counter, unique_books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc4d37e4-f364-488f-9a25-6f77e332ff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "调试信息：前5个用户和其书籍映射结果\n",
      "新用户ID: 0, 阅读记录数: 268\n",
      "  新书籍ID: 0, 书名: Matched (Matched, #1), 阅读时间: 1339225200.0\n",
      "  新书籍ID: 1, 书名: Firelight (Firelight, #1), 阅读时间: 1339657200.0\n",
      "  新书籍ID: 2, 书名: Thirteen Reasons Why, 阅读时间: 1340262000.0\n",
      "  新书籍ID: 3, 书名: The Fault in Our Stars, 阅读时间: 1341903600.0\n",
      "  新书籍ID: 4, 书名: City of Bones (The Mortal Instruments, #1), 阅读时间: 1355817600.0\n",
      "新用户ID: 1, 阅读记录数: 250\n",
      "  新书籍ID: 188, 书名: Vampire Academy (Vampire Academy, #1), 阅读时间: 1252631367.0\n",
      "  新书籍ID: 268, 书名: After, 阅读时间: 1252738800.0\n",
      "  新书籍ID: 210, 书名: Frostbite (Vampire Academy, #2), 阅读时间: 1259654400.0\n",
      "  新书籍ID: 269, 书名: A Match Made in High School, 阅读时间: 1260141161.0\n",
      "  新书籍ID: 270, 书名: Liar, 阅读时间: 1260141372.0\n",
      "新用户ID: 2, 阅读记录数: 156\n",
      "  新书籍ID: 490, 书名: The Looking Glass Wars (The Looking Glass Wars, #1), 阅读时间: 1191913200.0\n",
      "  新书籍ID: 491, 书名: Seeing Redd (The Looking Glass Wars, #2), 阅读时间: 1200816000.0\n",
      "  新书籍ID: 492, 书名: The Subtle Knife (His Dark Materials, #2), 阅读时间: 1204963200.0\n",
      "  新书籍ID: 493, 书名: Lucinda's Secret (The Spiderwick Chronicles, #3), 阅读时间: 1218783600.0\n",
      "  新书籍ID: 494, 书名: Graceling (Graceling Realm, #1), 阅读时间: 1221894000.0\n",
      "新用户ID: 3, 阅读记录数: 234\n",
      "  新书籍ID: 16, 书名: City of Fallen Angels (The Mortal Instruments, #4), 阅读时间: 1337238000.0\n",
      "  新书籍ID: 34, 书名: City of Lost Souls (The Mortal Instruments, #5), 阅读时间: 1337497200.0\n",
      "  新书籍ID: 623, 书名: The Girl of Fire and Thorns (Fire and Thorns, #1), 阅读时间: 1342249200.0\n",
      "  新书籍ID: 624, 书名: Thirteen Reasons Why, 阅读时间: 1344153939.0\n",
      "  新书籍ID: 625, 书名: The Perks of Being a Wallflower, 阅读时间: 1348038000.0\n",
      "新用户ID: 4, 阅读记录数: 176\n",
      "  新书籍ID: 188, 书名: Vampire Academy (Vampire Academy, #1), 阅读时间: 1338620400.0\n",
      "  新书籍ID: 350, 书名: Divergent (Divergent, #1), 阅读时间: 1339570800.0\n",
      "  新书籍ID: 365, 书名: Ten Things We Did, 阅读时间: 1344409200.0\n",
      "  新书籍ID: 210, 书名: Frostbite (Vampire Academy, #2), 阅读时间: 1349852400.0\n",
      "  新书籍ID: 155, 书名: Enchanted (Woodcutter Sisters #1; Books of Arilland #1), 阅读时间: 1355385600.0\n",
      "\n",
      "调试信息：前5个用户ID映射\n",
      "旧用户ID: 4a44f603cc3df339acc48590044a2db0, 新用户ID: 0\n",
      "旧用户ID: 5cca1dd30cd5a98c1c8e731839265ccf, 新用户ID: 1\n",
      "旧用户ID: 8bf745a1e2b3ec721ad079990111f114, 新用户ID: 2\n",
      "旧用户ID: 1c845473e18c23f917126cb29bc8d243, 新用户ID: 3\n",
      "旧用户ID: d76881f6f75216d6f25479114c66b62c, 新用户ID: 4\n",
      "\n",
      "调试信息：前5个书籍ID映射\n",
      "旧书籍ID: 7735333, 新书籍ID: 0, 书名: Matched (Matched, #1)\n",
      "旧书籍ID: 6448470, 新书籍ID: 1, 书名: Firelight (Firelight, #1)\n",
      "旧书籍ID: 1217100, 新书籍ID: 2, 书名: Thirteen Reasons Why\n",
      "旧书籍ID: 11870085, 新书籍ID: 3, 书名: The Fault in Our Stars\n",
      "旧书籍ID: 256683, 新书籍ID: 4, 书名: City of Bones (The Mortal Instruments, #1)\n",
      "处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 重新映射用户ID和书籍ID\n",
    "def remap_ids(data):\n",
    "    user_id_map = {}\n",
    "    book_id_map = {}\n",
    "    new_user_id = 0\n",
    "    new_book_id = 0\n",
    "    \n",
    "    remapped_data = {}\n",
    "\n",
    "    for user, books in data.items():\n",
    "        # 如果用户ID没有映射过，分配新的ID\n",
    "        if user not in user_id_map:\n",
    "            user_id_map[user] = new_user_id\n",
    "            new_user_id += 1\n",
    "        \n",
    "        new_books = []\n",
    "        for book in books:\n",
    "            book_id = book['book_id']\n",
    "            title = book['title']\n",
    "            # 如果书籍ID没有映射过，分配新的ID\n",
    "            if book_id not in book_id_map:\n",
    "                book_id_map[book_id] = (new_book_id, title)  # 保存book_id和title\n",
    "                new_book_id += 1\n",
    "            \n",
    "            # 替换书籍ID\n",
    "            new_books.append({\n",
    "                'book_id': book_id_map[book_id][0],  # 使用新ID\n",
    "                'title': title,\n",
    "                'read_at': book['read_at']\n",
    "            })\n",
    "        \n",
    "        # 使用新的用户ID和书籍ID\n",
    "        remapped_data[user_id_map[user]] = new_books\n",
    "\n",
    "    # 调试：打印前5个用户和书籍映射\n",
    "    print(\"\\n调试信息：前5个用户和其书籍映射结果\")\n",
    "    for i, (user, books) in enumerate(remapped_data.items()):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"新用户ID: {user}, 阅读记录数: {len(books)}\")\n",
    "        for book in books[:5]:  # 打印前5条阅读记录\n",
    "            print(f\"  新书籍ID: {book['book_id']}, 书名: {book['title']}, 阅读时间: {book['read_at']}\")\n",
    "    \n",
    "    return remapped_data, user_id_map, book_id_map\n",
    "\n",
    "# 生成 id2name.txt 文件\n",
    "def generate_id2name(book_id_map, output_file):\n",
    "    # 按书籍ID排序\n",
    "    sorted_books = sorted(book_id_map.items(), key=lambda x: x[1][0])  # 按照新的ID排序\n",
    "    with open(output_file, 'w') as f:\n",
    "        for book_id, (new_id, title) in sorted_books:\n",
    "            f.write(f\"{new_id}:: {title}\\n\")\n",
    "\n",
    "# 主流程\n",
    "file_path = 'processed_ya_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "\n",
    "# 重新映射用户ID和书籍ID\n",
    "remapped_data, user_id_map, book_id_map = remap_ids(data)\n",
    "\n",
    "# 将处理后的数据保存回 JSON 文件\n",
    "with open('remapped_ya_user_sessions.json', 'w') as f:\n",
    "    json.dump(remapped_data, f, indent=2)\n",
    "\n",
    "# 生成 id2name.txt 文件\n",
    "generate_id2name(book_id_map, 'id2name.txt')\n",
    "\n",
    "# 保存用户ID映射表\n",
    "with open('user_id_map.json', 'w') as f:\n",
    "    json.dump(user_id_map, f, indent=2)\n",
    "\n",
    "# 调试：检查映射表\n",
    "print(\"\\n调试信息：前5个用户ID映射\")\n",
    "for i, (old_id, new_id) in enumerate(user_id_map.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"旧用户ID: {old_id}, 新用户ID: {new_id}\")\n",
    "\n",
    "print(\"\\n调试信息：前5个书籍ID映射\")\n",
    "for i, (old_id, (new_id, title)) in enumerate(book_id_map.items()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"旧书籍ID: {old_id}, 新书籍ID: {new_id}, 书名: {title}\")\n",
    "\n",
    "print(\"处理完成，用户和书籍ID已更新，id2name.txt 文件已生成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "677d106f-9da9-47ae-99bb-cf04f75e3d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集生成完成，总记录数: 176176\n",
      "前5条训练集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0   [17, 89, 573, 461, 96, 633, 634, 685, 445, 2036]       10   642\n",
      "1  [100, 2373, 3432, 1428, 2388, 80, 81, 83, 3390...       10  3562\n",
      "2  [621, 1121, 454, 692, 721, 1899, 2203, 151, 13...       10   188\n",
      "3  [1071, 84, 471, 2270, 1326, 3413, 1163, 1414, ...       10  2503\n",
      "4  [3354, 374, 1165, 2038, 1119, 922, 937, 3669, ...       10   813\n",
      "验证/测试集生成完成，总记录数: 100\n",
      "前5条验证/测试集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [3345, 1914, 1305, 2032, 2571, 258, 771, 246, ...       10  1292\n",
      "1  [2463, 108, 2763, 736, 388, 238, 572, 1450, 70...       10   153\n",
      "2  [1465, 2431, 250, 2430, 742, 251, 837, 2677, 7...       10  2532\n",
      "3  [111, 2918, 2164, 439, 613, 1189, 709, 3195, 1...       10  2991\n",
      "4  [2836, 881, 1469, 880, 2292, 891, 767, 2972, 1...       10  2535\n",
      "验证/测试集生成完成，总记录数: 100\n",
      "前5条验证/测试集记录: \n",
      "                                                 seq  len_seq  next\n",
      "0  [1343, 875, 1554, 2511, 139, 145, 966, 1283, 5...       10  1635\n",
      "1  [2464, 976, 588, 1623, 3194, 176, 597, 1426, 8...       10  3576\n",
      "2  [1356, 1355, 2186, 1354, 1307, 2181, 889, 3248...       10   765\n",
      "3  [1082, 1314, 1674, 51, 2392, 946, 195, 1672, 3...       10    78\n",
      "4  [1238, 3504, 2793, 167, 148, 980, 839, 131, 13...       10  2534\n",
      "训练集 集合长度: 176176\n",
      "训练集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0   [17, 89, 573, 461, 96, 633, 634, 685, 445, 2036]       10   642\n",
      "1  [100, 2373, 3432, 1428, 2388, 80, 81, 83, 3390...       10  3562\n",
      "2  [621, 1121, 454, 692, 721, 1899, 2203, 151, 13...       10   188\n",
      "训练集 中 len_seq 的最小值: 1\n",
      "验证集 集合长度: 100\n",
      "验证集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [3345, 1914, 1305, 2032, 2571, 258, 771, 246, ...       10  1292\n",
      "1  [2463, 108, 2763, 736, 388, 238, 572, 1450, 70...       10   153\n",
      "2  [1465, 2431, 250, 2430, 742, 251, 837, 2677, 7...       10  2532\n",
      "验证集 中 len_seq 的最小值: 10\n",
      "测试集 集合长度: 100\n",
      "测试集 集合中前 3 条记录:\n",
      "                                                 seq  len_seq  next\n",
      "0  [1343, 875, 1554, 2511, 139, 145, 966, 1283, 5...       10  1635\n",
      "1  [2464, 976, 588, 1623, 3194, 176, 597, 1426, 8...       10  3576\n",
      "2  [1356, 1355, 2186, 1354, 1307, 2181, 889, 3248...       10   765\n",
      "测试集 中 len_seq 的最小值: 10\n",
      "数据集生成并保存完成。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 填充历史序列的函数，将 pad_item 填充到序列末尾\n",
    "def pad_history(itemlist, length, pad_item):\n",
    "    if len(itemlist) >= length:\n",
    "        return itemlist[-length:]\n",
    "    else:\n",
    "        return itemlist + [pad_item] * (length - len(itemlist))\n",
    "\n",
    "# # 生成训练集的函数，并过滤掉 len_seq = 0 的记录\n",
    "# def generate_train_sequences(data, length=10, pad_item=3934):\n",
    "#     state, len_state, action = [], [], []\n",
    "    \n",
    "#     for user_id, books in data.items():\n",
    "#         history = []\n",
    "#         for index, book in enumerate(books):\n",
    "#             s = list(history)  # 复制当前的历史记录\n",
    "#             if len(history) > 0:  # 只生成有效的历史序列\n",
    "#                 len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "#                 s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "#                 state.append(s)\n",
    "#                 action.append(book['book_id'])  # 预测的下一本书\n",
    "\n",
    "#             # 更新历史记录\n",
    "#             history.append(book['book_id'])\n",
    "    \n",
    "#     # 创建 DataFrame 并确保索引从 0 开始\n",
    "#     train_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "#     train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     # 打印一些调试信息\n",
    "#     print(f\"训练集生成完成，总记录数: {len(train_df)}\")\n",
    "#     print(f\"前5条训练集记录: \\n{train_df.head()}\")\n",
    "\n",
    "#     return train_df\n",
    "\n",
    "# 生成训练集的函数，并过滤掉 len_seq = 0 的记录，且打乱顺序\n",
    "def generate_train_sequences(data, length=10, pad_item=3934):\n",
    "    state, len_state, action = [], [], []\n",
    "    \n",
    "    for user_id, books in data.items():\n",
    "        history = []\n",
    "        for index, book in enumerate(books):\n",
    "            s = list(history)  # 复制当前的历史记录\n",
    "            if len(history) > 0:  # 只生成有效的历史序列\n",
    "                len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "                s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "                state.append(s)\n",
    "                action.append(book['book_id'])  # 预测的下一本书\n",
    "\n",
    "            # 更新历史记录\n",
    "            history.append(book['book_id'])\n",
    "    \n",
    "    # 创建 DataFrame 并确保索引从 0 开始\n",
    "    train_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 打乱数据\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # 打印一些调试信息\n",
    "    print(f\"训练集生成完成，总记录数: {len(train_df)}\")\n",
    "    print(f\"前5条训练集记录: \\n{train_df.head()}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "# 生成验证集和测试集的函数，并过滤掉 len_seq = 0 的记录\n",
    "def generate_test_sequences(data, length=10, pad_item=3934):\n",
    "    state, len_state, action = [], [], []\n",
    "    \n",
    "    for user_id, books in data.items():\n",
    "        history = [book['book_id'] for book in books]\n",
    "        \n",
    "        if len(history) > 1:\n",
    "            s = history[:-1]  # 最后一条作为预测目标，之前的作为历史记录\n",
    "        else:\n",
    "            s = []\n",
    "\n",
    "        if len(s) > 0:  # 只生成有效的历史序列\n",
    "            len_state.append(len(s) if len(s) < length else length)  # 保存历史序列的长度\n",
    "            s = pad_history(s, length, pad_item)  # 填充或截取历史序列\n",
    "\n",
    "            state.append(s)\n",
    "            action.append(history[-1])  # 最后一条作为预测目标\n",
    "    \n",
    "    # 创建 DataFrame 并确保索引从 0 开始\n",
    "    test_df = pd.DataFrame({'seq': state, 'len_seq': len_state, 'next': action})\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 打印一些调试信息\n",
    "    print(f\"验证/测试集生成完成，总记录数: {len(test_df)}\")\n",
    "    print(f\"前5条验证/测试集记录: \\n{test_df.head()}\")\n",
    "\n",
    "    return test_df\n",
    "\n",
    "# 检查数据集\n",
    "def check_data(df, name):\n",
    "    print(f\"{name} 集合长度: {len(df)}\")\n",
    "    print(f\"{name} 集合中前 3 条记录:\\n{df.head(3)}\")\n",
    "    print(f\"{name} 中 len_seq 的最小值: {df['len_seq'].min()}\")\n",
    "    assert df['len_seq'].min() > 0, f\"{name} 集合中有 len_seq = 0 的记录！\"\n",
    "\n",
    "# 主流程\n",
    "file_path = 'remapped_ya_user_sessions.json'\n",
    "data = load_data(file_path)\n",
    "\n",
    "# 假设用户按 8:1:1 划分为训练、验证和测试集\n",
    "total_users = list(data.keys())\n",
    "fractions = [0.8, 0.1, 0.1]\n",
    "train_users, val_users, test_users = np.split(total_users, [int(0.8*len(total_users)), int(0.9*len(total_users))])\n",
    "\n",
    "# 根据用户划分生成训练集、验证集和测试集\n",
    "train_data = {user: data[user] for user in train_users}\n",
    "val_data = {user: data[user] for user in val_users}\n",
    "test_data = {user: data[user] for user in test_users}\n",
    "\n",
    "# 生成并保存训练集\n",
    "train_df = generate_train_sequences(train_data)\n",
    "train_df.to_pickle('/workspace/teenager/train_data.df')\n",
    "\n",
    "# 生成并保存验证集\n",
    "val_df = generate_test_sequences(val_data)\n",
    "val_df.to_pickle('/workspace/teenager/val_data.df')\n",
    "\n",
    "# 生成并保存测试集\n",
    "test_df = generate_test_sequences(test_data)\n",
    "test_df.to_pickle('/workspace/teenager/test_data.df')\n",
    "\n",
    "# 检查生成的 DataFrame\n",
    "check_data(train_df, \"训练集\")\n",
    "check_data(val_df, \"验证集\")\n",
    "check_data(test_df, \"测试集\")\n",
    "\n",
    "print(\"数据集生成并保存完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9323f4bf-bc8f-4c23-8b6b-6d6c1c1daab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗完成，已保存到 /workspace/teenager/id2name_cleaned_no_brackets.txt\n",
      "清洗后条目数: 3934\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_id2name_remove_brackets(file_path, output_path):\n",
    "    cleaned_data = []\n",
    "    \n",
    "    # 定义一个正则表达式，匹配括号及其内部的内容\n",
    "    pattern = r'\\s*\\(.*?\\)'\n",
    "\n",
    "    # 读取并清洗数据\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('::')\n",
    "            if len(parts) == 2:  # 确保有 id 和 book_name 两部分\n",
    "                book_id, book_name = parts\n",
    "                if book_name:  # 确保书名不为空\n",
    "                    # 使用正则表达式去掉括号和其中的内容\n",
    "                    book_name_cleaned = re.sub(pattern, '', book_name).strip()\n",
    "                    cleaned_data.append(f\"{book_id}::{book_name_cleaned}\")\n",
    "    \n",
    "    # 保存清洗后的数据\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for line in cleaned_data:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    print(f\"清洗完成，已保存到 {output_path}\")\n",
    "    print(f\"清洗后条目数: {len(cleaned_data)}\")\n",
    "\n",
    "# 调用函数进行清洗\n",
    "file_path = '/workspace/teenager/id2name.txt'\n",
    "output_path = '/workspace/teenager/id2name_cleaned_no_brackets.txt'\n",
    "\n",
    "clean_id2name_remove_brackets(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae3034-e542-4a2b-978d-674335343254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
